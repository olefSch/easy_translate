{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Easy NLP Translate - A Simple and Efficient Translation Library","text":"<p>This library provides an easy-to-use interface for translating text using both local transformer models and large language models (LLMs) from popular providers. It is designed to simplify the translation process while ensuring high-quality results through careful model evaluation also shown in the documantion.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Translation with a local transformer model selected based on our evaltuaion</li> <li>LLM Translation using common provider with a costum prompt or based on our prompty lirary</li> <li>Access to own distilled model</li> </ul>"},{"location":"#installation-instructions","title":"\ud83d\udce6 Installation &amp; Instructions","text":""},{"location":"#installion","title":"Installion","text":"<p>Install the package from pypi:</p> <pre><code>pip install easy-nlp-translate\n</code></pre>"},{"location":"#example-project-where-easy-nlp-transalte-gets-used","title":"Example Project where easy-nlp-transalte gets used","text":"<p>This Repo shows an example project where <code>easy-nlp-translate</code> is used to translate text in a web application. It demonstrates how to integrate the library into a larger project and utilize its features effectively. The project includes a streamlit frontend which enables users to create subtitles for their videos. Those subtitles get translated using <code>easy-nlp-translate</code> and then saved to a database.</p>"},{"location":"#license","title":"\ud83e\uddfe License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"evaluation/config/","title":"Configuration","text":"<p>This section describes the global configuration used in the translation evaluation framework. The configuration defines available models, evaluation targets, dataset settings, and language-specific code mappings for each model type.</p>"},{"location":"evaluation/config/#overview","title":"Overview","text":"<p>All configuration parameters are stored in <code>config.py</code> and <code>language_mappings.yaml</code>. These define:</p> <ul> <li>The available translation models</li> <li>The models to be evaluated in the current run</li> <li>Dataset split settings</li> <li>Output directory paths</li> <li>Language code mappings by model and language pair</li> </ul>"},{"location":"evaluation/config/#model-registry","title":"Model Registry","text":"<p>Translation models are registered in the <code>MODEL_REGISTRY</code> dictionary. Each key corresponds to a model name used in evaluation scripts, and the value is either a translator class or a pre-configured partial constructor.</p> <pre><code>MODEL_REGISTRY = {\n    \"nllb\":     NllbTranslator,\n    \"m2m100\":   M2M100Translator,\n    \"mbart50\":  MBartTranslator,\n    \"marian\":   MarianTranslator,\n    \"llama3.2\": partial(LLMTranslator, model_name=\"llama3.2:3b\"),\n    \"llama3.1\": partial(LLMTranslator, model_name=\"llama3.1:8b\"),\n    \"gemma\":    partial(LLMTranslator, model_name=\"gemma3:4b\"),\n    \"phi3\":     partial(LLMTranslator, model_name=\"phi3:3.8b\"),\n    \"mistral\":  partial(LLMTranslator, model_name=\"mistral:7b\"),\n}\n</code></pre>"},{"location":"evaluation/config/#models-to-evaluate","title":"Models to Evaluate","text":"<p>The <code>MODELS_TO_EVALUATE</code> list determines which models are run during evaluation:</p> <pre><code>MODELS_TO_EVALUATE = [\n    \"mistral\",\n    \"m2m100\",\n    \"marian\",\n    \"nllb\",\n    \"mbart50\",\n    \"llama3.1\",\n    \"llama3.2\",\n    \"gemma\",\n    \"phi3\",\n]\n</code></pre>"},{"location":"evaluation/config/#dataset-configuration","title":"Dataset Configuration","text":"<p>Evaluation is based on the WMT-19 dataset using a controlled subset:</p> <pre><code>DATASET_NAME = \"wmt19\"\nDATASET_SPLIT = \"train[:1000]\"\n</code></pre> <ul> <li><code>DATASET_NAME</code> defines the source dataset.</li> <li><code>DATASET_SPLIT</code> specifies a sample of 1,000 sentence pairs per language pair for evaluation.</li> </ul>"},{"location":"evaluation/config/#output-directory","title":"Output Directory","text":"<p>Generated evaluation reports are saved to:</p> <pre><code>OUTPUT_DIR = Path(\"reports\")\n</code></pre>"},{"location":"evaluation/config/#language-code-mappings","title":"Language Code Mappings","text":"<p>Each translation model requires specific source/target language code formats. These mappings are defined in the external YAML file:</p> <pre><code>LANGUAGE_MAPPING_PATH = Path(\"configs/language_mappings.yaml\")\n</code></pre> <p>Sample structure from language_mappings.yaml:</p> <p><pre><code>de-en:\n  nllb:    {source: deu_Latn,  target: eng_Latn}\n  mbart50: {source: de_DE,     target: en_XX}\n  marian:  {source: de,        target: en}\n  mistral: {source: German,    target: English}\n</code></pre> This format is repeated for each language pair (fi-en, gu-en, etc.), and each entry is model-specific. This allows exact alignment with the required format for each tokenizer and model.</p>"},{"location":"evaluation/config/#summary","title":"Summary","text":"Config Parameter Purpose <code>MODEL_REGISTRY</code> Maps model names to translator classes <code>MODELS_TO_EVALUATE</code> List of models to run <code>DATASET_NAME</code> Defines which dataset to use <code>DATASET_SPLIT</code> Limits dataset to manageable size <code>OUTPUT_DIR</code> Destination for reports and results <code>LANGUAGE_MAPPING_PATH</code> Path to language code definitions (YAML)"},{"location":"evaluation/evaluator/","title":"TranslationEvaluator","text":""},{"location":"evaluation/evaluator/#evaluation.translation_evaluator.TranslationEvaluator","title":"<code>TranslationEvaluator</code>","text":"<p>Evaluate translation models against reference translations.</p> <p>This class orchestrates evaluation of multiple translation models computing metrics like BLEU and METEOR on provided datasets. Results can be retrieved programmatically or saved as reports.</p> Typical usage <p>evaluator = TranslationEvaluator() evaluator.register_model('model1', translator1) results = evaluator.evaluate(     inputs, references, model_names=['model1']) evaluator.generate_report('results.csv', models=['model1'])</p>"},{"location":"evaluation/evaluator/#evaluation.translation_evaluator.TranslationEvaluator.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the evaluator with default metrics.</p>"},{"location":"evaluation/evaluator/#evaluation.translation_evaluator.TranslationEvaluator.evaluate","title":"<code>evaluate(inputs, references, model_names=None)</code>","text":"<p>Evaluate registered models on inputs against reference translations.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>List of source texts to translate.</p> required <code>references</code> <code>List[str]</code> <p>List of corresponding reference translations.</p> required <code>model_names</code> <code>Optional[List[str]]</code> <p>Subset of registered model names to evaluate. If None, evaluates all registered models.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, float]]</code> <p>Dict[str, Dict[str, float]]: Mapping from model name to a dict of metric scores.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs and references lengths differ.</p> <code>KeyError</code> <p>If a specified model or metric is not registered.</p>"},{"location":"evaluation/evaluator/#evaluation.translation_evaluator.TranslationEvaluator.generate_report","title":"<code>generate_report(file_path, models=None)</code>","text":"<p>Save and print evaluation report for BLEU &amp; METEOR.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Destination CSV file path.</p> required <code>models</code> <code>Optional[Union[str, List[str]]]</code> <p>Single model name, list of names, or None for all.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no evaluation results are available.</p>"},{"location":"evaluation/evaluator/#evaluation.translation_evaluator.TranslationEvaluator.register_model","title":"<code>register_model(name, model)</code>","text":"<p>Register a translation model for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique identifier for the model.</p> required <code>model</code> <code>BaseTranslator</code> <p>Instance implementing BaseTranslator interface.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty or model is None.</p>"},{"location":"evaluation/metrics/","title":"Evaluation Metrics","text":"<p>Translation quality was evaluated using two widely accepted automatic metrics: BLEU and METEOR. Each captures different aspects of translation performance and helps compare outputs from traditional NMT models and modern LLMs.</p>"},{"location":"evaluation/metrics/#bleu-bilingual-evaluation-understudy","title":"BLEU (Bilingual Evaluation Understudy)","text":"<p>BLEU is a precision-based metric that measures n-gram overlap between a model\u2019s translation and a reference translation.</p> <ul> <li>Best suited for literal translations with consistent word choices</li> <li>Performs reliably for high-resource language pairs with rigid grammar structures</li> </ul> <p>Scoring:</p> <ul> <li>Range: 0.0 to 1.0 (expressed as a decimal or percentage)</li> <li>Higher scores indicate closer surface-level matches.</li> </ul> Score Interpretation 0.40+ Excellent (near-human, fluent output) 0.30\u20130.40 Good quality 0.20\u20130.30 Understandable but flawed 0.10\u20130.20 Low quality or partially incorrect &lt; 0.10 Very poor translation or off-topic <p>Limitations:</p> <ul> <li>Does not account for synonyms or word order</li> <li>Penalizes legitimate paraphrasing, which may bias against LLMs</li> </ul>"},{"location":"evaluation/metrics/#meteor-metric-for-evaluation-of-translation-with-explicit-ordering","title":"METEOR (Metric for Evaluation of Translation with Explicit ORdering)","text":"<p>METEOR offers a more nuanced evaluation by considering:</p> <ul> <li>Stemming (e.g., \u201crun\u201d vs. \u201crunning\u201d)</li> <li>Synonyms (e.g., \u201cchild\u201d vs. \u201ckid\u201d)</li> <li>Word reordering</li> </ul> <p>Scoring:</p> <ul> <li>Range: 0.0 to 1.0</li> <li>METEOR scores may be lower than BLEU in literal translations, but often higher for generated outputs due to its handling of synonyms, stems, and flexible word order</li> </ul> Score Interpretation 0.60+ Very strong (fluent and faithful) 0.50\u20130.60 Good quality 0.40\u20130.50 Mixed (may contain awkward phrasing) &lt; 0.40 Often disfluent or semantically incorrect <p>Strengths:</p> <ul> <li>Captures semantic similarity, useful for LLM outputs</li> <li>More aligned with human judgment of fluency and meaning</li> </ul>"},{"location":"evaluation/metrics/#notes-on-usage","title":"Notes on Usage","text":"<ul> <li>Both metrics are used for relative comparison across models</li> <li>Neither is perfect on its own - human evaluation is often needed for nuanced distinctions</li> <li>METEOR tends to favor natural fluency, while BLEU rewards surface-level accuracy</li> </ul>"},{"location":"evaluation/model/","title":"Model Implementation","text":""},{"location":"evaluation/model/#evaluation.models.base_translator.BaseTranslator","title":"<code>BaseTranslator</code>","text":"<p>Abstract base class for translator models. Ensures a consistent interface for all translators, whether local LLM-based or via an API.</p>"},{"location":"evaluation/model/#evaluation.models.base_translator.BaseTranslator.translate","title":"<code>translate(text)</code>  <code>abstractmethod</code>","text":"<p>Translate a single piece of text from the source language to the target language.</p>"},{"location":"evaluation/model/#evaluation.models.mBART_50_model_translator.MBartTranslator","title":"<code>MBartTranslator</code>","text":"<p>Translator using Hugging Face\u2019s mBART-50 model for multilingual translation.</p>"},{"location":"evaluation/model/#evaluation.models.mBART_50_model_translator.MBartTranslator.__init__","title":"<code>__init__(source_lang, target_lang, device='cpu', max_length=512, num_beams=4, tokenizer_kwargs=None, model_kwargs=None)</code>","text":"<p>Initialize the mBART-50 translator.</p> <p>Parameters:</p> Name Type Description Default <code>source_lang</code> <code>str</code> <p>Source language code (e.g. \"en_XX\").</p> required <code>target_lang</code> <code>str</code> <p>Target language code (e.g. \"de_DE\").</p> required <code>device</code> <code>Union[str, device]</code> <p>\"cpu\", \"cuda\", or a torch.device. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>max_length</code> <code>int</code> <p>Maximum length of generated sequences. Defaults to 512.</p> <code>512</code> <code>num_beams</code> <code>int</code> <p>Number of beams for beam\u2010search. Defaults to 4.</p> <code>4</code> <code>tokenizer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Extra kwargs for <code>MBart50TokenizerFast.from_pretrained</code>. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Extra kwargs for <code>MBartForConditionalGeneration.from_pretrained</code>. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If source_lang or target_lang are empty, or if max_length or num_beams are not positive.</p>"},{"location":"evaluation/model/#evaluation.models.mBART_50_model_translator.MBartTranslator.translate","title":"<code>translate(text)</code>","text":"<p>Translate a single sentence using the mBART-50 model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input sentence in the source language.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Translated sentence.</p> <p>Raises:</p> Type Description <code>TranslationError</code> <p>If <code>text</code> is empty or generation fails.</p>"},{"location":"evaluation/model/#evaluation.models.llm_model_translator.LLMTranslator","title":"<code>LLMTranslator</code>","text":"<p>Translator that drives any Ollama\u2011hosted model via the Ollama Python client.</p>"},{"location":"evaluation/model/#evaluation.models.llm_model_translator.LLMTranslator.__init__","title":"<code>__init__(model_name='llama3.1:8b', num_predict=512, source_lang='English', target_lang='German', stop=None, client=None, prompt_template=None)</code>","text":"<p>Initialize an Ollama-based translator.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Ollama model ID (e.g. \"llama3.1:8b\").</p> <code>'llama3.1:8b'</code> <code>num_predict</code> <code>int</code> <p>Maximum number of tokens to predict per call.</p> <code>512</code> <code>source_lang</code> <code>str</code> <p>Name of the source language.</p> <code>'English'</code> <code>target_lang</code> <code>str</code> <p>Name of the target language.</p> <code>'German'</code> <code>stop</code> <code>Optional[List[str]]</code> <p>Optional list of stop sequences; defaults to [\"\u2014\"].</p> <code>None</code> <code>client</code> <code>Optional[Client]</code> <p>Optional pre-configured Ollama Client; if None, constructs a new one.</p> <code>None</code> <code>prompt_template</code> <code>Optional[str]</code> <p>Optional prompt template with placeholders {source_lang}, {target_lang}, {text}. If None uses DEFAULT_TEMPLATE.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_name, source_lang, or target_lang are empty, or if num_predict is not positive.</p>"},{"location":"evaluation/model/#evaluation.models.llm_model_translator.LLMTranslator.translate","title":"<code>translate(text)</code>","text":"<p>Translate the given text via the Ollama LLM.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A single sentence to translate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The translated sentence (stripped of surrounding whitespace).</p> <p>Raises:</p> Type Description <code>TranslationError</code> <p>On any failure from the Ollama client.</p>"},{"location":"evaluation/overview/","title":"Evaluation Overview","text":"<p>The Easy Translate Evaluation framework is designed to benchmark the performance of machine translation models and large language models across multiple languages, using consistent metrics and datasets.</p>"},{"location":"evaluation/overview/#goal","title":"Goal","text":"<p>To provide a reproducible, flexible, and extensible pipeline for:</p> <ul> <li>Evaluating translation quality using standard metrics</li> <li>Comparing traditional NMT models with modern local LLM-based translators</li> <li>Identify an effective model for local, offline translation</li> </ul>"},{"location":"evaluation/overview/#dataset-wmt-19","title":"Dataset: WMT-19","text":"<p>The WMT-19 (Workshop on Machine Translation 2019) dataset serves as the primary benchmark for this evaluation. It includes professionally curated parallel corpora across many language pairs and is widely adopted in machine translation research.</p> <p>Key attributes:</p> <ul> <li>High-quality human references for English translations</li> <li>Standardized formatting, compatible with MT toolkits</li> <li>A sample of 1,000 sentence pairs per language was selected to manage computational requirements</li> </ul>"},{"location":"evaluation/overview/#language-pairs","title":"Language Pairs","text":"<p>Translations were evaluated from six source languages into English, using subsets of the WMT-19 dataset:</p> <ul> <li>\ud83c\udde9\ud83c\uddea German \u2192 English (<code>de-en</code>)</li> <li>\ud83c\uddeb\ud83c\uddee Finnish \u2192 English (<code>fi-en</code>)</li> <li>\ud83c\uddee\ud83c\uddf3 Gujarati \u2192 English (<code>gu-en</code>)</li> <li>\ud83c\uddf1\ud83c\uddf9 Lithuanian \u2192 English (<code>lt-en</code>)</li> <li>\ud83c\uddf7\ud83c\uddfa Russian \u2192 English (<code>ru-en</code>)</li> <li>\ud83c\udde8\ud83c\uddf3 Chinese \u2192 English (<code>zh-en</code>)</li> </ul>"},{"location":"evaluation/overview/#tools-frameworks","title":"Tools &amp; Frameworks","text":"<p>Two model types were included in the evaluation:</p> <ul> <li>Transformer-based machine translation models, accessed through the Hugging Face Transformers library</li> <li>Locally-run LLMs such as LLaMA, Mistral, Gemma, and Phi, deployed via Ollama</li> </ul> <p>This setup enables comparison between domain-specific MT systems and general-purpose LLMs in translation tasks.</p>"},{"location":"evaluation/overview/#models-evaluated","title":"Models Evaluated","text":"Model Type Size (Parameters) Description <code>mbart50</code> MT-specific ~610M Facebook\u2019s multilingual encoder-decoder model for 50+ languages. <code>marian</code> MT-specific ~280M Fast, language-pair\u2013specific model from Helsinki-NLP. <code>nllb</code> MT-specific 600M (distilled) Facebook\u2019s \"No Language Left Behind\" (distilled version). <code>m2m100</code> MT-specific ~418M Facebook\u2019s many-to-many multilingual model supporting 100+ languages. <code>mistral</code> General LLM 7B Decoder-only open LLM (Mistral AI); not trained for translation. <code>llama3.1</code> General LLM 8B Meta\u2019s LLaMA 3 model (8B), with instruction tuning and multilingual potential. <code>llama3.2</code> General LLM 3B Smaller LLaMA 3 variant (3B); weaker performance on translation tasks. <code>gemma</code> General LLM 4B Google\u2019s compact open-source LLM, multilingual-capable. <code>phi3</code> General LLM 3.8B Microsoft\u2019s small LLM designed for efficiency; weak on translation."},{"location":"evaluation/overview/#language-mappings","title":"Language Mappings","text":"<p>Each model architecture requires different language code formats. The following table illustrates the mappings used for the <code>de-en</code> (German to English) direction:</p> Model Type Source Lang Target Lang <code>nllb</code> <code>deu_Latn</code> <code>eng_Latn</code> <code>m2m100</code> <code>de</code> <code>en</code> <code>mbart50</code> <code>de_DE</code> <code>en_XX</code> <code>marian</code> <code>de</code> <code>en</code> <code>llama/gemma/phi3/mistral</code> <code>German</code> <code>English</code> <p>Equivalent mappings were defined for all language pairs during evaluation in the configurations.</p>"},{"location":"evaluation/overview/#evaluation-results","title":"Evaluation Results","text":""},{"location":"evaluation/overview/#bleu-scores","title":"BLEU Scores","text":"base_model de-en fi-en gu-en lt-en ru-en zh-en gemma 0.2173 0.1929 0.0980 0.1756 0.2208 0.2253 llama3.1 0.2054 0.1730 0.0951 0.1000 0.1953 0.1957 llama3.2 0.1757 0.1314 0.0215 0.0768 0.1741 0.1806 m2m100 0.2223 0.2163 0.0193 0.2484 0.2072 0.2026 marian 0.2899 0.2946 NaN NaN 0.1930 0.2573 mbart50 0.2859 0.2928 0.8241 0.5610 0.2307 0.2721 mistral 0.1812 0.1144 0.0103 0.0388 0.1934 0.1803 nllb 0.2726 0.2570 0.1026 0.2570 0.2141 0.2476 phi3 0.1520 0.0318 0.0000 0.0099 0.0951 0.1204"},{"location":"evaluation/overview/#meteor-scores","title":"METEOR Scores","text":"base_model de-en fi-en gu-en lt-en ru-en zh-en gemma 0.5098 0.4902 0.3822 0.4187 0.4517 0.5118 llama3.1 0.5060 0.4724 0.3897 0.3820 0.4291 0.4921 llama3.2 0.4708 0.4113 0.3160 0.3071 0.3952 0.4691 m2m100 0.5106 0.5107 0.0724 0.5055 0.4219 0.4725 marian 0.5754 0.5048 NaN NaN 0.4955 0.5662 mbart50 0.5609 0.5384 0.6526 0.8063 0.4937 0.5777 mistral 0.4772 0.3782 0.0862 0.2632 0.4135 0.4740 nllb 0.5455 0.4375 0.3934 0.5194 0.4784 0.5166 phi3 0.3825 0.2192 0.0584 0.1779 0.3214 0.3948"},{"location":"evaluation/overview/#averaged-scores","title":"Averaged Scores","text":"base_model BLEU METEOR mbart50 0.4122 0.6139 marian 0.2588 0.5465 nllb 0.2252 0.5106 gemma 0.1883 0.4607 m2m100 0.1860 0.4405 llama3.1 0.1608 0.4452 llama3.2 0.1267 0.3949 mistral 0.1182 0.3493 phi3 0.0682 0.2693"},{"location":"evaluation/usage/","title":"Usage Guide","text":"<p>This section explains how to run the evaluation script, configure model selection, and generate reports for translation model performance.</p> <p>The main entry point for the framework is <code>main.py</code>. It handles configuration loading, dataset preparation, translation execution, and report generation.</p>"},{"location":"evaluation/usage/#running-the-evaluation","title":"Running the Evaluation","text":"<p>To start the evaluation, run the following command from the project root:</p> <pre><code>python main.py\n</code></pre> <p>This will:</p> <ol> <li>Load the language mappings from <code>configs/language_mappings.yaml</code></li> <li>Instantiate a <code>TranslationEvaluator</code> object</li> <li>Evaluate each model listed in <code>MODELS_TO_EVALUATE</code> across supported language pairs</li> <li>Generate a <code>.csv</code> report for each model-language pair in the <code>report/</code> directory</li> </ol>"},{"location":"evaluation/usage/#configuration-overview","title":"Configuration Overview","text":"<p>All paths and evaluation parameters are defined in <code>config.py</code></p> <pre><code>MODELS_TO_EVALUATE = [\"mbart50\", \"nllb\", \"mistral\"]\nMODEL_REGISTRY = {\n    \"nllb\": NllbTranslator,\n    \"mbart50\": MBartTranslator,\n    \"mistral\": partial(LLMTranslator, model_name=\"mistral:7b\")\n}\nDATASET_NAME = \"wmt19\"\nDATASET_SPLIT = \"train[:1000]\"\nOUTPUT_DIR = Path(\"reports\")\nLANGUAGE_MAPPING_PATH = Path(\"configs/language_mappings.yaml\")\n</code></pre> <p>To evaluate different models or use a smaller dataset split, update these values before running <code>main.py</code>.</p>"},{"location":"evaluation/usage/#what-happens-internally","title":"What Happens Internally","text":"<p>Load Language Mappings</p> <pre><code>mappings = load_language_mappings(LANGUAGE_MAPPING_PATH)\n</code></pre> <p>Loads a YAML file that maps each language pair to the model-specific source and target language codes.</p> <p>Loop Through Models and Language Pairs</p> <p>Each model in <code>MODELS_TO_EVALUATE</code> is evaluated on each language pair for which it has a mapping: <pre><code>for model_name in models:\n    for lang_pair in mappings:\n        if model_name in mappings[lang_pair]:\n            ...\n</code></pre></p> <p>Load Dataset</p> <p>Translations and reference texts are pulled from Hugging Face datasets: <pre><code>ds = load_dataset(dataset_name, lang_pair, split=split)\ninputs = [ex[\"translation\"][src_lang] for ex in ds]\nrefs = [ex[\"translation\"][tgt_lang] for ex in ds]\n</code></pre></p> <p>Initialize Translator</p> <p>Each translator class is dynamically loaded from <code>MODEL_REGISTRY</code>: <pre><code>translator_cls = MODEL_REGISTRY[model_name]\ntranslator = translator_cls(source_lang=src_code, target_lang=tgt_code)\n</code></pre></p> <p>Evaluate Translations</p> <p>The <code>TranslationEvaluator</code> handles translation and scoring: <pre><code>evaluator.register_model(model_id, translator)\nevaluator.evaluate(inputs, refs, model_names=[model_id])\n</code></pre></p> <p>Generate Report</p> <p>Results are saved as CSV files in the output directory: <pre><code>report_file = output_dir / f\"{model_id}_report.csv\"\nevaluator.generate_report(report_file, models=model_id)\n</code></pre> Each report includes scores for BLEU and METEOR.</p>"},{"location":"evaluation/usage/#output-files","title":"Output Files","text":"<p>After a successful run, the following will be available:</p> <p><pre><code>reports/\n\u251c\u2500\u2500 mbart50_de-en_report.csv\n\u251c\u2500\u2500 nllb_fi-en_report.csv\n\u251c\u2500\u2500 mistral_ru-en_report.csv\n...\n</code></pre> Each file contains the metric results for a specific model and language pair.</p>"},{"location":"evaluation/usage/#common-issues","title":"Common Issues","text":"Issue Explanation <code>KeyError: 'language_mappings'</code> Check the YAML file formatting and top-level key. <code>FileNotFoundError</code> Make sure the paths in <code>config.py</code> are correct. <code>Unknown model 'xyz'</code> Ensure all listed models exist in <code>MODEL_REGISTRY</code>. Hugging Face dataset error The dataset or language pair may not be supported. Try a different one or check availability on huggingface.co/datasets."},{"location":"package_docs/languages_support/","title":"Supported Languages for easy-nlp-translate","text":"<p>This page lists all the languages currently supported by the <code>easy-nlp-translate</code> package. You can use the provided language codes when initializing a translator or specifying source/target languages.</p> Language Name Language Code Example Sentence Afrikaans <code>af</code> Goeie m\u00f4re. Arabic <code>ar</code> \u0635\u0628\u0627\u062d \u0627\u0644\u062e\u064a\u0631 Azerbaijani <code>az</code> Sabah\u0131n\u0131z xeyir. Bengali <code>bn</code> \u09b6\u09c1\u09ad \u09b8\u0995\u09be\u09b2\u0964 Czech <code>cs</code> Dobr\u00e9 r\u00e1no. German <code>de</code> Guten Morgen. English <code>en</code> Good morning. Spanish <code>es</code> Buenos d\u00edas. Estonian <code>et</code> Tere hommikust. Persian <code>fa</code> \u0635\u0628\u062d \u0628\u062e\u06cc\u0631 Finnish <code>fi</code> Hyv\u00e4\u00e4 huomenta. French <code>fr</code> Bonjour. Galician <code>gl</code> Bos d\u00edas. Gujarati <code>gu</code> \u0ab6\u0ac1\u0aad \u0aaa\u0acd\u0ab0\u0aad\u0abe\u0aa4. Hebrew <code>he</code> \u05d1\u05d5\u05e7\u05e8 \u05d8\u05d5\u05d1 Hindi <code>hi</code> \u0938\u0941\u092a\u094d\u0930\u092d\u093e\u0924\u0964 Croatian <code>hr</code> Dobro jutro. Indonesian <code>id</code> Selamat pagi. Italian <code>it</code> Buongiorno. Japanese <code>ja</code> \u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\u3002 Georgian <code>ka</code> \u10d3\u10d8\u10da\u10d0 \u10db\u10e8\u10d5\u10d8\u10d3\u10dd\u10d1\u10d8\u10e1\u10d0. Kazakh <code>kk</code> \u049a\u0430\u0439\u044b\u0440\u043b\u044b \u0442\u0430\u04a3. Khmer <code>km</code> \u17a2\u179a\u17bb\u178e\u179f\u17bd\u179f\u17d2\u178f\u17b8\u17d4 Korean <code>ko</code> \uc88b\uc740 \uc544\uce68\uc785\ub2c8\ub2e4. Lithuanian <code>lt</code> Labas rytas. Latvian <code>lv</code> Labr\u012bt. Macedonian <code>mk</code> \u0414\u043e\u0431\u0440\u043e \u0443\u0442\u0440\u043e. Malayalam <code>ml</code> \u0d38\u0d41\u0d2a\u0d4d\u0d30\u0d2d\u0d3e\u0d24\u0d02. Mongolian <code>mn</code> \u04e8\u0433\u043b\u04e9\u04e9\u043d\u0438\u0439 \u043c\u044d\u043d\u0434. Marathi <code>mr</code> \u0936\u0941\u092d \u0938\u0915\u093e\u0933. Burmese <code>my</code> \u1019\u1004\u103a\u1039\u1002\u101c\u102c\u1014\u1036\u1014\u1000\u103a\u1001\u1004\u103a\u1038\u1015\u102b\u104b Nepali <code>ne</code> \u0936\u0941\u092d \u092a\u094d\u0930\u092d\u093e\u0924\u0964 Dutch <code>nl</code> Goedemorgen. Polish <code>pl</code> Dzie\u0144 dobry. Pashto <code>ps</code> \u0633\u062d\u0631 \u0645\u0648 \u067e\u062e\u064a\u0631 Portuguese <code>pt</code> Bom dia. Romanian <code>ro</code> Bun\u0103 diminea\u021ba. Russian <code>ru</code> \u0414\u043e\u0431\u0440\u043e\u0435 \u0443\u0442\u0440\u043e. Sinhala <code>si</code> \u0dc3\u0dd4\u0db7 \u0d8b\u0daf\u0dd1\u0dc3\u0db1\u0d9a\u0dca. Slovene <code>sl</code> Dobro jutro. Swedish <code>sv</code> God morgon. Swahili <code>sw</code> Habari za asubuhi. Tamil <code>ta</code> \u0b95\u0bbe\u0bb2\u0bc8 \u0bb5\u0ba3\u0b95\u0bcd\u0b95\u0bae\u0bcd. Telugu <code>te</code> \u0c36\u0c41\u0c2d\u0c4b\u0c26\u0c2f\u0c02. Thai <code>th</code> \u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35\u0e04\u0e23\u0e31\u0e1a Tagalog <code>tl</code> Magandang umaga. Turkish <code>tr</code> G\u00fcnayd\u0131n. Ukrainian <code>uk</code> \u0414\u043e\u0431\u0440\u043e\u0433\u043e \u0440\u0430\u043d\u043a\u0443. Urdu <code>ur</code> \u0635\u0628\u062d \u0628\u062e\u064a\u0631 Vietnamese <code>vi</code> Ch\u00e0o bu\u1ed5i s\u00e1ng. Xhosa <code>xh</code> Molo. Chinese (Simplified) <code>zh-cn</code> \u65e9\u4e0a\u597d\u3002"},{"location":"package_docs/llm_translation/","title":"Models for LLM translations","text":"<p>coming ...</p>"},{"location":"package_docs/non_llm_translation/","title":"Models for Non-LLM translations","text":""},{"location":"package_docs/non_llm_translation/#src.huggingface_models.mbart.MBARTTranslator","title":"<code>MBARTTranslator</code>","text":"<p>MBART model for translation. It supports many-to-many translation across multiple languages and is locally usable.</p>"},{"location":"package_docs/non_llm_translation/#src.huggingface_models.mbart.MBARTTranslator.__init__","title":"<code>__init__(target_lang, source_lang=None, device='cuda' if torch.cuda.is_available() else 'cpu', max_length=512, num_beams=4, tokenizer_kwargs=None, model_kwargs=None)</code>","text":"<p>Initializes the MBARTTranslator.</p> <p>All parameters are used to configure the underlying Hugging Face model and tokenizer as defined in the <code>HuggingFaceTranslator</code> base class.</p> <p>Parameters:</p> Name Type Description Default <code>target_lang</code> <code>str</code> <p>The target language code for translation (e.g., 'de_DE' for German, using MBART specific codes if applicable, or generic codes if <code>CODE_MAPPER</code> handles conversion).</p> required <code>source_lang</code> <code>Optional[str]</code> <p>The source language code for translation (e.g., 'en_XX' for English). If not provided, the MBART model's default behavior for source language detection applies.</p> <code>None</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>The device (e.g., \"cpu\", \"cuda\", \"mps\") on which the MBART model and tokenizer will be loaded. Defaults to \"cuda\" if a CUDA-enabled GPU is available, otherwise \"cpu\".</p> <code>'cuda' if is_available() else 'cpu'</code> <code>max_length</code> <code>Optional[int]</code> <p>The maximum sequence length for generated translations by MBART. Defaults to 512.</p> <code>512</code> <code>num_beams</code> <code>Optional[int]</code> <p>The number of beams for beam search decoding with MBART. Defaults to 4.</p> <code>4</code> <code>tokenizer_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional keyword arguments for the MBART tokenizer. Defaults to None.</p> <code>None</code> <code>model_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Additional keyword arguments for the MBART model. Defaults to None.</p> <code>None</code>"},{"location":"package_docs/non_llm_translation/#src.huggingface_models.mbart.MBARTTranslator.detect_language","title":"<code>detect_language(text)</code>","text":"<p>Detects the language of the given text using langdetect.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text whose language is to be detected.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The detected language code (e.g., 'en', 'fr').</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the text is empty or invalid for detection.</p> <code>LangDetectException</code> <p>If language detection by the <code>langdetect</code> library fails for other reasons (e.g., text too short, no features).</p> <code>ValueError</code> <p>If the detected language is not in <code>self.LANGUAGE_CODES</code>.</p>"},{"location":"package_docs/non_llm_translation/#src.huggingface_models.mbart.MBARTTranslator.translate","title":"<code>translate(text)</code>","text":"<p>Translate the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to translate.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The translated text.</p>"},{"location":"package_docs/non_llm_translation/#src.huggingface_models.mbart.MBARTTranslator.translate_batch","title":"<code>translate_batch(texts)</code>","text":"<p>Translate a batch of texts from source language to target language.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list</code> <p>A list of texts to be translated.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of translated texts.</p>"},{"location":"package_docs/overview/","title":"General Instructions on How to Use the Package","text":"<p>This package allows you to create translator objects for translating text according to your needs.</p>"},{"location":"package_docs/overview/#available-translators","title":"Available Translators","text":"<p>The following translators are available, each identified by a unique ID:</p> Translator ID (<code>translator_name</code>) Description MBart50 <code>mbart</code> A multilingual translation model supporting 50 languages. It can also run locally without external dependencies. This is also the best machine translator out of the evaluation we did. Gemini <code>gemini</code> Coming soon... GPT <code>gpt</code> Coming soon... Claude <code>claude</code> Coming soon..."},{"location":"package_docs/overview/#basic-usage","title":"Basic Usage","text":"<p>To use a translator, you first need to initialize it using its <code>translator_name</code> ID.</p> Initialize the Translator<pre><code>from easy_nlp_translate import initialize_translator\n\n# Initialize the translator using its ID\ntranslator = initialize_translator(\n    translator_name=\"mbart\", \n    source_lang=\"en\", \n    target_lang=\"de\"\n)\n</code></pre> <p>Initialization Parameters</p> <p>For more details on <code>source_lang</code>, <code>target_lang</code>, and any other parameters specific to each translator (like API keys or model paths), please refer to the detailed documentation for the respective translator classes.</p> <p>Once the translator object is created, you can use its <code>translate</code> method to translate your text.</p> Translating Text<pre><code>translated_text = translator.translate(\"My dog is beautiful.\")\n\nprint(translated_text)\n# Expected output (will vary based on the model and target language):\n# Mein Hund ist wundersch\u00f6n.\n</code></pre> <p>You can also translate multiple texts in a batch if the translator supports it (e.g., the <code>translate_batch</code> method). Please check the specific translator's documentation for availability and usage of batch translation.</p>"},{"location":"package_docs/overview/#llm-translator-initialization","title":"LLM Translator Initialization","text":"<p>comming ...</p>"}]}